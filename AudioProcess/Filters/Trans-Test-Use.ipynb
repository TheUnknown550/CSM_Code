{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tempfile as tmp\n",
    "import soundfile as sf\n",
    "from scipy import signal, stats\n",
    "from scipy.io import wavfile\n",
    "from typing import Tuple, Union\n",
    "from sklearn import model_selection, preprocessing\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, utils, metrics\n",
    "from tensorflow_addons import activations as tfa_activations\n",
    "from keras_nlp import layers as nlp_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\AppData\\Local\\Temp\\ipykernel_26892\\2002002543.py:2: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  (sampling_rate, wave_data) = wavfile.read(filename=path)\n"
     ]
    }
   ],
   "source": [
    "def trim_wav_file(path: str, st: int = 0, ed: int = 10) -> np.ndarray:\n",
    "    (sampling_rate, wave_data) = wavfile.read(filename=path)\n",
    "    st_sample: int = int(st * sampling_rate)\n",
    "    ed_sample: int = int(ed * sampling_rate)\n",
    "    wave_data = wave_data[st_sample:ed_sample]\n",
    "    del st_sample, ed_sample\n",
    "    return wave_data\n",
    "\n",
    "def get_signal_list(path: str) -> list[np.ndarray]:\n",
    "    file_names: list[str] = glob.glob(pathname=f\"{path}/*.wav\")\n",
    "    sample_list: list[np.ndarray] = []\n",
    "    for file_name in file_names:\n",
    "        sample: np.ndarray = trim_wav_file(path=file_name, st=0, ed=10)\n",
    "        sample_list.append(sample)\n",
    "        del sample\n",
    "    del file_name, file_names\n",
    "    return sample_list\n",
    "\n",
    "def combine_signal_list(signal_list: list[np.ndarray], mn_len: int) -> np.ndarray:\n",
    "    sample_list: list[np.ndarray] = []\n",
    "    for sgnal in signal_list:\n",
    "        sample_list.append(sgnal[:mn_len])\n",
    "    del sgnal\n",
    "    return np.stack(sample_list, axis=0)\n",
    "\n",
    "orig_list: list[np.ndarray] = get_signal_list(path=\"Dataset/Signal\")\n",
    "noise_list: list[np.ndarray] = get_signal_list(path=\"Dataset/Noise\")\n",
    "\n",
    "mn: int = 1e9\n",
    "for orig in orig_list:\n",
    "    mn = min(mn, orig.shape[0])\n",
    "del orig\n",
    "for noise in noise_list:\n",
    "    mn = min(mn, noise.shape[0])\n",
    "del noise\n",
    "\n",
    "orig: np.ndarray = combine_signal_list(orig_list, mn)\n",
    "noise: np.ndarray = combine_signal_list(noise_list, mn)[:, :, 0]\n",
    "del mn, orig_list, noise_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = signal.resample_poly(orig, up=1, down=3, axis=1)\n",
    "noise = signal.resample(noise, num=1898, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_x_y_div(orig: np.ndarray, noise: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    x_list: list[np.ndarray] = []\n",
    "    y_list: list[np.ndarray] = []\n",
    "    x_avg_list: list[np.ndarray] = []\n",
    "    x_sd_list: list[np.ndarray] = []\n",
    "    for i in range(0, orig.shape[0]):\n",
    "        for j in range(0, noise.shape[0]):\n",
    "            combi: np.ndarray = orig[i] + noise[j]\n",
    "            x_list.append(np.stack([combi, noise[j]], axis=1))\n",
    "            x_avg_list.append(np.stack([np.average(combi, axis=0), np.average(noise[j], axis=0)], axis=0)[np.newaxis, :])\n",
    "            x_sd_list.append(np.stack([np.std(combi, axis=0), np.std(noise[j], axis=0)], axis=0)[np.newaxis, :])\n",
    "            y_list.append(orig[i])\n",
    "            del combi\n",
    "        del j\n",
    "    del i\n",
    "    x: np.ndarray = np.stack(x_list, axis=0)\n",
    "    y: np.ndarray = np.stack(y_list, axis=0)\n",
    "    x_avg: np.ndarray = np.stack(x_avg_list, axis=0)\n",
    "    x_sd: np.ndarray = np.stack(x_sd_list, axis=0)\n",
    "    return (x, y, x_avg, x_sd)\n",
    "\n",
    "(x, y_old, x_avg, x_sd) = gen_x_y_div(orig, noise)\n",
    "x = (x - x_avg) / x_sd\n",
    "y = (y_old - x_avg[:, :, [0]]) / x_sd[:, :, [0]]\n",
    "y = y[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, x_test, y_train, y_test) = model_selection.train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objs: dict = {\n",
    "    \"keras_nlp>PositionEmbedding\": nlp_layers.PositionEmbedding,\n",
    "    \"keras_nlp>TransformerEncoder\": nlp_layers.TransformerEncoder,\n",
    "    \"keras_nlp>TransformerDecoder\": nlp_layers.TransformerDecoder,\n",
    "    \"Addons>mish\": tfa_activations.mish,\n",
    "}\n",
    "model = keras.models.load_model(\"Model.h5\", custom_objects=custom_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 40s 3s/step\n",
      "2717.949315058769\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "y_pred = y_pred * x_sd[:, :, [0]] + x_avg[:, :, [0]]\n",
    "rmse = metrics.RootMeanSquaredError(dtype=np.float64)\n",
    "rmse.update_state(y_old[:, :, np.newaxis], y_pred)\n",
    "print(rmse.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  58.80050987  -59.16861121   58.80053848 ...   58.80052894\n",
      "    58.80052894   58.80052894]\n",
      " [  59.1090056   -59.0184014    59.1090056  ...   59.1090438\n",
      "    59.1090438    59.10903425]\n",
      " [  60.38994695  -58.85277977   60.38994695 ...   60.38996623\n",
      "    60.38997587   60.38996623]\n",
      " ...\n",
      " [-206.31796678 -356.49782204 -206.31793035 ... -206.31796678\n",
      "  -206.31791821 -206.31791821]\n",
      " [-170.16791673 -379.24298919 -170.16786603 ... -170.16791673\n",
      "  -170.16791673 -170.16786603]\n",
      " [   1.68703159 -503.43634803    1.68703159 ...    1.6871541\n",
      "     1.6871541     1.68703159]]\n",
      "[[  66.41497  334.75644  798.471   ...   66.55102 -150.31052 -126.06131]\n",
      " [  66.41497  334.75644  798.471   ...   66.55102 -150.31052 -126.06131]\n",
      " [  66.41497  334.75644  798.471   ...   66.55102 -150.31052 -126.06131]\n",
      " ...\n",
      " [-180.37022 -327.0981  -415.47083 ... -416.4989  -543.7176  -347.04242]\n",
      " [-180.37022 -327.0981  -415.47083 ... -416.4989  -543.7176  -347.04242]\n",
      " [-180.37022 -327.0981  -415.47083 ... -416.4989  -543.7176  -347.04242]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:, :, 0])\n",
    "print(y_old)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
